# Pytorch Pruning and Interpretability
_(Can Pruning Further Improve Interpretability in Adversarially Robust Computer Vision Systems)_

This research investigates the impact of pruning on interpretability in computer vision models. Using the CIFAR-10 dataset, we conducted experiments by comparing the saliency maps of an adversarially robust model before and after pruning. For this comparison, we introduce a new approach, the Differences method. Our findings indicate that pruning has positive effect on interpretability. These results provide valuable insights into the relationship between model compression and understanding the reasons behind the model's decisions. Our future work will explore further the potential of pruning to improve the interpretability of adversarially robust models, with the aim of developing more transparent and reliable computer vision systems.

